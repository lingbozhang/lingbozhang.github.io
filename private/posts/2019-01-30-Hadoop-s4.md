---
title:  "Hadoop Source Code Analysis-- Part 3"
categories: [Distributed Systems]
tags: [Hadoop]
mathjax: true
---

## Setup a HDFS study project
Setup a simple Java project with Maven:
```bash
mvn archetype:generate -DarchetypeGroupId=org.apache.maven.archetypes -DarchetypeArtifactId=maven-archetype-quickstart -DarchetypeVersion=1.4
```
Configure the pom.xml of the project:
{% highlight XML %}
  <name>main</name>
  <!-- FIXME change it to the project's website -->
  <url>http://lingbozh.github.io</url>

  <properties>
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    <maven.compiler.source>1.8</maven.compiler.source>
    <maven.compiler.target>1.8</maven.compiler.target>
  </properties>
  <!-- Add hadoop-client dependencies-->
  <dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-client</artifactId>
    <version>2.7.2</version>
  </dependency>
{% endhighlight %}

Copy <a href="#CD.java"> CompressDemo.java </a> into ./src/main/java/io/github/lingbozh/CompressDemo.java and test the code (recommend using vscode), if the HDFS and the project is successfully setup, you will see three files generated from your current user directory (you can get current user directory by invoking System.getProperty("user.dir") in Java).

## Hadoop I/O



















## Appendix
<details><summary id="CD.java">CompressDemo.java</summary>
{% highlight java %}
package io.github.lingbozh;

import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.lang.Class;
import java.io.BufferedWriter;
import java.io.FileWriter;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.io.compress.CompressionCodec;
import org.apache.hadoop.io.compress.CompressionCodecFactory;
import org.apache.hadoop.io.compress.CompressionOutputStream;
import org.apache.hadoop.io.compress.Compressor;
import org.apache.hadoop.io.compress.CompressorStream;
import org.apache.hadoop.io.compress.GzipCodec;
import org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater;
import org.apache.hadoop.util.ReflectionUtils;

public class CompressDemo {	
	public static void compress(String method) throws ClassNotFoundException, IOException {
		File fileIn = new File("README.txt");
		InputStream in =  new FileInputStream(fileIn);
		
		Class<?> codecClass = Class.forName(method);
		
		Configuration conf = new Configuration();		
		CompressionCodec codec = (CompressionCodec)
		    ReflectionUtils.newInstance(codecClass, conf);
		
		File fileOut = new File("README.txt"+codec.getDefaultExtension());
		fileOut.delete();
		OutputStream out =  new FileOutputStream(fileOut);
		
		CompressionOutputStream cout =
			codec.createOutputStream(out);
		
		IOUtils.copyBytes(in, cout, 4096, false);
		
		in.close();
		cout.close();
	}	
	
	public static void decompress(File file) throws IOException {		
		Configuration conf = new Configuration();
		CompressionCodecFactory factory = new CompressionCodecFactory(conf);
		
		CompressionCodec codec = factory.getCodec(new Path(file.getName()));
		
		if( codec == null ) {
			System.out.println("Cannot find codec for file "+file);
			return;
		}

		File fileOut = new File(file.getName()+".txt");
		
		InputStream in = codec.createInputStream( new FileInputStream(file) );
		OutputStream out =  new FileOutputStream(fileOut);
		
		IOUtils.copyBytes(in, out, 4096, false);
		
		in.close();
		out.close();
	}

	static final int compressorOutputBufferSize=100;
	
	public static void compressor() throws IOException {
        File fileIn = new File("README.txt");
        if(!fileIn.exists()) System.out.println("file does not exist!");
		InputStream in =  new FileInputStream(fileIn);
		int datalength = in.available();
		byte[] inbuf = new byte[datalength];     
		in.read(inbuf, 0, datalength);
		in.close();

		byte[] outbuf = new byte[compressorOutputBufferSize];
		
		Compressor compressor=new BuiltInZlibDeflater();
		
		int step=100;
		int inputPos=0;
		int putcount=0;
		int getcount=0;
		int compressedlen=0;
		
		while(inputPos < datalength) {
			int len=(datalength-inputPos>=step)? step:datalength-inputPos;
			compressor.setInput(inbuf, inputPos, len );
			putcount++;
		    while (!compressor.needsInput()) {
		    	compressedlen=compressor.compress(outbuf, 0, compressorOutputBufferSize);
		    	if(compressedlen>0) {
		    		getcount++;
		    	}
		    }
			inputPos+=step;
		}
			
		compressor.finish();

//      // loop by compressor.compress() return value
//		compressedlen=compressor.compress(outbuf, 0, compressorOutputBufferSize);
//		while( compressedlen > 0 ) {
//			getcount++;
//			compressedlen=compressor.compress(outbuf, 0, compressorOutputBufferSize);
//		}
		
		while(!compressor.finished()) {
			getcount++;
		    compressor.compress(outbuf, 0, compressorOutputBufferSize);
		}
		
		System.out.println("Compress "+compressor.getBytesRead()+" bytes into "+compressor.getBytesWritten());
		System.out.println("put "+putcount+" times and get "+getcount+" times");
		
		compressor.end();		
	}
	
	public static void compressorStream() throws IOException {
		File fileIn = new File("README.txt");
		InputStream in =  new FileInputStream(fileIn);
				
		File fileOut = new File("README.txt.stream.gz");
		fileOut.delete();
		OutputStream out =  new FileOutputStream(fileOut);
		
		GzipCodec codec=new GzipCodec();
		codec.setConf(new Configuration());
		CompressorStream cout = new CompressorStream(out, codec.createCompressor(), 10);
		
		IOUtils.copyBytes(in, cout, 10, false);
		
		in.close();
		cout.close();
	}

	public static void main(String[] args) {
		try {
            //compressorStream();
            
            File file = new File(System.getProperty("user.dir") + "README.txt");
            if (!file.exists()) {
                file.createNewFile();
                String contents = "Hello World ! From Lingbo Zhang";
                BufferedWriter writer = new BufferedWriter(new FileWriter("README.txt"));
                writer.write(contents);
                writer.close();
            }
            
			compressor();
			
			//compress("org.apahe.hadoop.io.compress.DefaultCodec");
			compress("org.apache.hadoop.io.compress.GzipCodec");
			//compress("org.apache.hadoop.io.compress.BZip2Codec");

			//decompress(new File("README.txt.bz2"));
			//decompress(new File("README.txt.deflate"));
            //decompress(new File("README.txt"));
            decompress(new File("README.txt.gz"));

		} catch (ClassNotFoundException e) {
			e.printStackTrace();
		} catch (IOException e) {
			e.printStackTrace();
		}

	}
}
{% endhighlight %}
</details>




